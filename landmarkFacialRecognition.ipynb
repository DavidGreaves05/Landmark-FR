{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23962,"status":"ok","timestamp":1685700535364,"user":{"displayName":"David Greaves","userId":"13952683110847939778"},"user_tz":-60},"id":"QsgvAPRYuOZx","outputId":"6c42d2fd-5069-45ac-8468-a074dbdf2627"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import numpy as np\n","from numpy.linalg import norm\n","import os\n","import matplotlib.pyplot as plt\n","import cv2\n","import re\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["The below file paths will most likely need to be changed to run this project."],"metadata":{"id":"JxdIWXHQig34"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_qpZndv9ObW"},"outputs":[],"source":["landmarks1 = \"/content/drive/MyDrive/Final_Year_Project/final_year_project/Images/Landmark/landmark_list_part1.txt\"\n","landmarks2 = \"/content/drive/MyDrive/Final_Year_Project/final_year_project/Images/Landmark/landmark_list_part2.txt\"\n","landmarks3 = \"/content/drive/MyDrive/Final_Year_Project/final_year_project/Images/Landmark/landmark_list_part3.txt\"\n","!unzip \"/content/drive/MyDrive/Final_Year_Project/final_year_project/Images/UTKface_Alignedcropped.zip\" -d \"/content\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wPG94uq_llhp"},"outputs":[],"source":["cropped_image_labels = []\n","image_var = []\n","cropped_file_re = re.compile('\\d{1,3}_[01]_[0-4]_\\d+\\..+[jpg]+') # Expected format for the filenames\n","\n","# Loop to extract labels from the images\n","for filename in os.listdir(\"/content/UTKface_Aligned&cropped/UTKFace\"):\n","  if cropped_file_re.match(filename) != None: # Non-match will return None\n","    temp_cropped_image_labels = []\n","    temp_cropped_image_labels.append(filename)\n","    split = filename.split('_', 3) # Split filename by the '_' character to get the labels\n","    temp_cropped_image_labels.append(int(split[0])) # Age label\n","    temp_cropped_image_labels.append(int(split[1])) # Gender label\n","    temp_cropped_image_labels.append(int(split[2])) # Ethnicity label\n","    cropped_image_labels.append(temp_cropped_image_labels)\n","    image_var.append(cv2.imread(os.path.join(\"/content/UTKface_Aligned&cropped/UTKFace\", filename)))"]},{"cell_type":"markdown","metadata":{"id":"KACzRMVhKPIw"},"source":["# Pruning Function\n","\n","Function created with the purpose of removing specified values from the image set to allow a bias to be created"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TzEBRqFtEX5Y"},"outputs":[],"source":["def removeValuesControl(lableList, images, ageMin, ethnicityBias):\n","  \"\"\"\n","  Remove specified values and remove the inate bias within the dataset\n","  :param labelList: list of lists containing the filenames and labels for each image\n","  :param images: list of Ndarrays containing images\n","  :param ageMin: int value that states minimum age allowed in dataset\n","  :param ethnicityBias: int value that states the ethnicity label to be removed from the dataset\n","  \"\"\"\n","  tempPrunedLableList = []\n","  tempPrunedImages = []\n","  whiteHolder = []\n","  whiteImageHolder = []\n","  combinedHolder = []\n","  combinedImageHolder = []\n","  prunedLableList = []\n","  prunedImages = []\n","  # Removal of images that fall under the specified values of ageMin and ethnicityBias\n","  for i in range(len(lableList)):\n","    if (lableList[i][1] > ageMin) and (lableList[i][3] != ethnicityBias):\n","      tempPrunedLableList.append(lableList[i])\n","      tempPrunedImages.append(images[i])\n","\n","  # Separation of ethicity label 0\n","  for j in range(len(tempPrunedLableList)):\n","    if tempPrunedLableList[j][3] == 0:\n","      whiteHolder.append(tempPrunedLableList[j])\n","      whiteImageHolder.append(tempPrunedImages[j])\n","    else:\n","      combinedHolder.append(tempPrunedLableList[j])\n","      combinedImageHolder.append(tempPrunedImages[j])\n","\n","  # Halving the number of images of ethnicity label 0 in the dataset to remove the inate bias in the dataset\n","  for k in range(len(whiteHolder)):\n","    if k % 2 == 0:\n","      prunedLableList.append(whiteHolder[k])\n","      prunedImages.append(whiteImageHolder[k])\n","\n","  for n in range(len(combinedHolder)):\n","    prunedLableList.append(combinedHolder[n])\n","    prunedImages.append(combinedImageHolder[n])\n","\n","  return prunedLableList, prunedImages\n","\n","def removeValuesBias(labelList, images, ageMin, ethnicity):\n","  \"\"\"\n","  Remove all values from the dataset apart from those that >= to ageMin and share a label with ethnicity\n","  :param labelList: list of lists containing the filenames and labels for each image\n","  :images: list of Ndarrays containing images\n","  :ageMin: int value that states minimum age allowed in dataset\n","  :ethnicity: int value that states the ethnicity label to be removed from the dataset\n","  \"\"\"\n","  prunedLabelList = []\n","  prunedImages = []\n","  # Removal of images that fall under the specified value of ageMin and that don't match with ethnicity\n","  for i in range(len(labelList)):\n","    if (labelList[i][1] > ageMin) and (labelList[i][3] == ethnicity):\n","      prunedLabelList.append(labelList[i])\n","      prunedImages.append(images[i])\n","\n","  return prunedLabelList, prunedImages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"moQhw41mBIg7"},"outputs":[],"source":["control_labels, control_images = removeValuesControl(cropped_image_labels, image_var, 17, 4)\n","white_labels, white_images = removeValuesBias(cropped_image_labels, image_var, 17, 0)\n","black_labels, black_images = removeValuesBias(cropped_image_labels, image_var, 17, 1)\n","asian_labels, asian_images = removeValuesBias(cropped_image_labels, image_var, 17, 2)\n","indian_labels, indian_images = removeValuesBias(cropped_image_labels, image_var, 17, 3)\n","other_labels, other_images = removeValuesBias(cropped_image_labels, image_var, 17, 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rczFUO_wB1D8"},"outputs":[],"source":["def concatLists(list1, list2, list3):\n","  \"\"\"\n","  Concatenate lists together\n","  :param list1: list to be concatenated\n","  :param list2: list to be concatenated\n","  :param list3: list to be concatenated\n","  \"\"\"\n","  concatList = []\n","  for line1 in list1:\n","    concatList.append(line1)\n","\n","  for line2 in list2:\n","    concatList.append(line2)\n","\n","  for line3 in list3:\n","    concatList.append(line3)\n","\n","  return concatList"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3SSk7nItmFl6"},"outputs":[],"source":["open_landmark1 = open(landmarks1, 'r')\n","open_landmark2 = open(landmarks2, 'r')\n","open_landmark3 = open(landmarks3, 'r')\n","landmark1_str = open_landmark1.readlines()\n","landmark2_str = open_landmark2.readlines()\n","landmark3_str = open_landmark3.readlines()\n","\n","landmark_list = concatLists(landmark1_str, landmark2_str, landmark3_str) # Contactenate all three .txt files containing the landmark info\n","\n","landmark_info = []\n","# Iterate through each line of the landmark info .txt files\n","for string in landmark_list:\n","  split_str = string.split(' ') # Split the string of the current line on the ' ' character\n","  clean_split_str = []\n","  temp_landmark_info = []\n","  temp_landmarks = np.zeros((136,), dtype = int)\n","  # Remove the new line '\\n' character from the list as it isn't needed\n","  for val in split_str:\n","    if val != '\\n':\n","      clean_split_str.append(val)\n","  # Check to catch the one line that has an error where there is a space between the filename and the file extension making the list 1 more in length\n","  if len(clean_split_str) == 138:\n","    image_name = clean_split_str[0]\n","    extension = clean_split_str[1]\n","    true_image_name = image_name + extension # Combine filename and file extension\n","    temp_landmark_info.append(true_image_name)\n","    temp_count = 0\n","    # Iterate through the coordinates for a given image and inputs them to a pre-initialised Ndarray full of zeros\n","    for j in clean_split_str[2:137]:\n","      temp_landmarks[temp_count] = j\n","      temp_count += 1\n","\n","    reshaped_temp_landmarks = temp_landmarks.reshape((68, 2)) # Reshape the coordinates so that they can actually be read as coordinates\n","    temp_landmark_info.append(reshaped_temp_landmarks)\n","    landmark_info.append(temp_landmark_info)\n","    continue\n","\n","  temp_landmark_info.append(clean_split_str[0])\n","  count = 0\n","  # Iterate through the coordinates for a given image and inputs them to a pre-initialised Ndarray full of zeros\n","  for i in clean_split_str[1:136]:\n","    temp_landmarks[count] = int(i)\n","    count += 1\n","\n","  reshaped_temp_landmarks = temp_landmarks.reshape((68, 2)) # Reshape the coordinates so that they can actually be read as coordinates\n","  temp_landmark_info.append(reshaped_temp_landmarks)\n","  landmark_info.append(temp_landmark_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WT2SGHbc6uab"},"outputs":[],"source":["def imagePreprocessing(images):\n","  \"\"\"\n","  Applies preprocessing techniques to a given list of images\n","  :param images: list of Ndarrays containing images\n","  \"\"\"\n","  images = np.asarray(images) # Changing the images list to a Ndarray in function for performance reasons\n","  image32 = images.astype(\"float32\") / 255 # Normalise the images by making all of the pixel values being 0-1 instead of 0-255\n","  greyScaleImages = []\n","  for image in image32:\n","    greyScaleImages.append(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)) # Greyscale the images\n","\n","  greyScaleImages = np.asarray(greyScaleImages, dtype=\"float32\") # Change list of Ndarrays containing images to a Ndarray of Ndarrays contianing images\n","\n","  return greyScaleImages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lOGy0j1uO_Ea"},"outputs":[],"source":["grey_control_images = imagePreprocessing(control_images)\n","grey_white_images = imagePreprocessing(white_images)\n","grey_black_images = imagePreprocessing(black_images)\n","grey_asian_images = imagePreprocessing(asian_images)\n","grey_indian_images = imagePreprocessing(indian_images)\n","grey_other_images = imagePreprocessing(other_images)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gfvixMQZO_Ea"},"outputs":[],"source":["# Train, test variables with random state given so that both arrays are shuffled identically\n","train_control_labels, test_control_labels, train_control_images, test_control_images = train_test_split(control_labels,\n","                                                                                                        grey_control_images,\n","                                                                                                        train_size=0.66, random_state=2)\n","train_white_labels, test_white_labels, train_white_images, test_white_images = train_test_split(white_labels,\n","                                                                                                grey_white_images,\n","                                                                                                train_size=0.7, random_state=2)\n","train_black_labels, test_black_labels, train_black_images, test_black_images = train_test_split(black_labels,\n","                                                                                                grey_black_images,\n","                                                                                                train_size=0.7, random_state=2)\n","train_asian_labels, test_asian_labels, train_asian_images, test_asian_images = train_test_split(asian_labels,\n","                                                                                                grey_asian_images,\n","                                                                                                train_size=0.7,random_state=2)\n","train_indian_labels, test_indian_labels, train_indian_images, test_indian_images = train_test_split(indian_labels,\n","                                                                                                    grey_indian_images,\n","                                                                                                    train_size=0.7, random_state=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nBomLL3_O_Eb"},"outputs":[],"source":["def trainingLandmarks(imageLabels, landmarkInfo, images):\n","  \"\"\"\n","  Obtain the landmark coordinates of the images and order them in the same order as the images\n","  :param imageLabels: list of lists containing the filename and labels for the images\n","  :param landmarkInfo: list of Strings and Ndarrays containing filenames and landmark coordinates respectively\n","  :param images: Ndarray of Ndarrays containing images\n","  \"\"\"\n","  trainLandmarkInfo = []\n","  for j in range(len(imageLabels)):\n","    # Check for if the length of the trainLandmarkInfo list matches the value of j\n","    if len(trainLandmarkInfo) != j:\n","        trainLandmarkInfo.append(np.zeros((68, 2), dtype=int)) # Add a temporary value into the array so that the rest of the values are ordered correctly\n","        print('Index value: ', j - 1)\n","\n","    for m in range(len(landmarkInfo)):\n","      # Check for if the filenames match in the 2 lists\n","      if landmarkInfo[m][0] in imageLabels[j][0]:\n","        trainLandmarkInfo.append(landmarkInfo[m][1])\n","\n","  for k in range(len(trainLandmarkInfo)):\n","    # Check for the temporary value if it was added\n","    if np.all(trainLandmarkInfo[k] == 0):\n","        images = np.delete(images, k, 0) # Removal of the image as it is useless without its landmark coordinates\n","        trainLandmarkInfo = np.delete(trainLandmarkInfo, k, 0) # Removal of the temporary value\n","        break # Temp fix. Not a fix that will work if more than one errors occur\n","\n","  for n in range(len(trainLandmarkInfo)):\n","    # Check for any potential values that are of an unexpected type or shape\n","    if (type(trainLandmarkInfo[n]) != np.ndarray) or (trainLandmarkInfo[n].shape != (68, 2)):\n","      print(f'Index type: ', type(trainLandmarkInfo[n]), '\\nIndex shape: ', trainLandmarkInfo[n].shape)\n","\n","  trainLandmarkInfo = np.asarray(trainLandmarkInfo, dtype=int)\n","\n","  return trainLandmarkInfo, images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fjNDwEFPO_Eb"},"outputs":[],"source":["train_control_landmarks, train_control_images = trainingLandmarks(train_control_labels, landmark_info, train_control_images)\n","test_control_landmarks, test_control_images = trainingLandmarks(test_control_labels, landmark_info, test_control_images)\n","\n","train_white_landmarks, train_white_images = trainingLandmarks(train_white_labels, landmark_info, train_white_images)\n","test_white_landmarks, test_white_images = trainingLandmarks(test_white_labels, landmark_info, test_white_images)\n","\n","train_black_landmarks, train_black_images = trainingLandmarks(train_black_labels, landmark_info, train_black_images)\n","test_black_landmarks, test_black_images = trainingLandmarks(test_black_labels, landmark_info, test_black_images)\n","\n","train_asian_landmarks, train_asian_images = trainingLandmarks(train_asian_labels, landmark_info, train_asian_images)\n","test_asian_landmarks, test_asian_images = trainingLandmarks(test_asian_labels, landmark_info, test_asian_images)\n","\n","train_indian_landmarks, train_indian_images = trainingLandmarks(train_indian_labels, landmark_info, train_indian_images)\n","test_indian_landmarks, test_indian_images = trainingLandmarks(test_indian_labels, landmark_info, test_indian_images)"]},{"cell_type":"markdown","source":["# Pathways for model weights\n","To load the weights for a specific bias change the file pathway in the model.load_weights() function to one of the pathways below. The pathways below are not the complete pathways as the full pathway may vary depending on the file structure you have the weights stored in.\n","\n","## Control\n",".../CNN_Weights/Control_bias_weights/control_checkpoint\n","\n","## White bias\n",".../CNN_Weights/White_bias_weights/white_checkpoint_2\n","\n","## Black bias\n",".../CNN_Weights/Black_bias_weights/black_checkpoint_2\n","\n","## Asian bias\n",".../CNN_Weights/Asian_bias_weights/asian_checkpoint_2\n","\n","## Indian bias\n",".../CNN_Weights/Indian_bias_weights/indian_checkpoint\n","\n","\n"],"metadata":{"id":"M0BQ3992sZRm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZbF-rIlDO_Ed"},"outputs":[],"source":["# Define the CNN model\n","model = keras.Sequential([\n","    layers.Conv2D(32, (3,3), padding='same', strides=2, activation='selu', bias_initializer=keras.initializers.VarianceScaling, input_shape=(200, 200, 1)), # Output = 32, 100, 100\n","    layers.MaxPooling2D((2,2)), # Output = 32, 50, 50\n","    layers.Conv2D(64, (3,3), bias_initializer=keras.initializers.VarianceScaling, activation='selu'), # Output = 64, 48, 48\n","    layers.MaxPooling2D((2,2)), # Output = 64, 24, 24\n","    layers.Conv2D(128, (3,3), bias_initializer=keras.initializers.VarianceScaling, activation='selu'), # Output = 128, 22, 22\n","    layers.MaxPooling2D((2,2)), # Output = 128, 11, 11\n","    layers.Flatten(), # Output = 15488\n","    layers.Dense(3872, bias_initializer=keras.initializers.VarianceScaling, activation='selu'),\n","    layers.Dense(1290, bias_initializer=keras.initializers.VarianceScaling, activation='selu'),\n","    layers.Dense(645, bias_initializer=keras.initializers.VarianceScaling, activation='selu'),\n","    layers.Dense(322, bias_initializer=keras.initializers.VarianceScaling, activation='selu'),\n","    layers.Dense(68*2) # Output layer of size (136) so that can be reshaped to (68, 2)\n","])\n","\n","# Compile the model\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001,\n","                                                weight_decay=0.00001),\n","                                                loss='mean_squarred_error',\n","                                                metrics=['accuracy'])\n","model.load_weights(\"/content/drive/MyDrive/Final_Year_Project/final_year_project/CNN_Weights/Control_bias_weights/control_checkpoint\")"]},{"cell_type":"code","source":["# Checkpoint in training to adjust learning rate\n","checkpoint_dir = \"/content/drive/MyDrive/Final_Year_Project/final_year_project/CNN_Weights/Control_bias_weights/control_checkpoint_2\"\n","model_weight_checkpoint = keras.callbacks.ModelCheckpoint(filepath=checkpoint_dir,\n","                                                          monitor='loss',\n","                                                          save_weights_only=True,\n","                                                          save_best_only=True,\n","                                                          mode='min',\n","                                                          verbose=1)\n","\n","# Train the model\n","history = model.fit(train_control_images, np.reshape(train_control_landmarks,\n","                                               (train_control_landmarks.shape[0], 136)),\n","                                               validation_split=0.2, epochs=30,\n","                                               batch_size=64,\n","                                               callbacks=[model_weight_checkpoint])"],"metadata":{"id":"UucG1bcmIffn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def landmarkSimilarity(yHat, y):\n","  \"\"\"\n","  Calculate the cosine similarity of the y-hat values against the y values\n","  :param yHat: Ndarray of Ndarrays containing the predicted landmark coordinates of given images\n","  :param y: list of Strings and Ndarrays containing filenames and landmark coordinates of images\n","  \"\"\"\n","  similarityIndexes = []\n","\n","  for i in range(len(yHat)):\n","    tempSimilarityValues = []\n","    for j in range(len(y)):\n","      tempSimilarityValues.append(np.dot(yHat[i], (y[j][1].flatten()))/(norm(yHat[i])*norm((y[j][1].flatten())))) # Calculate cosine similarity of y-hat and y\n","\n","    maxIndex = tempSimilarityValues.index(max(tempSimilarityValues)) # Obtain the index value of the landmark coordinates with the greatest cosine similarity\n","    similarityIndexes.append(maxIndex)\n","\n","  return similarityIndexes\n","\n","def predictionSample(images, numImages):\n","  \"\"\"\n","  Generate a set of random images\n","  :param images: Ndarray of Ndarrays containing images\n","  :param numImages: int value determining the amount of images to get\n","  \"\"\"\n","  randList = []\n","  imageSample = []\n","  for n in range(numImages):\n","    randList.append(np.random.randint(0, len(images))) # Obtaining random index values\n","\n","  for index in randList:\n","    imageSample.append(images[index]) # Obtaining the images determined by the index values\n","\n","  imageSample = np.asarray(imageSample) # Turning list of Ndarrays containing images into Ndarray of Ndarrays containing images\n","\n","  return imageSample\n","\n","def similarImages(imageIndex, labelList, images, landmarkInfo):\n","  \"\"\"\n","  Obtains the images determined by the index values obtained through the landmarkSimilarity function\n","  :param imageIndex: list of int values representing index value of the landmarkInfo parameter\n","  :param labelList: list of lists storing the filename and lables of the images\n","  :param images: list of Ndarrays storing images\n","  :param landmarkInfo: list of Strings and Ndarrays storing filenames and landmark coordinates respectively\n","  \"\"\"\n","  similarImages = []\n","  rgbSimilarImages = []\n","  for index in imageIndex:\n","    for i in range(len(labelList)):\n","      # Check for where in labelList the filename in landmarkInfo exists\n","      if landmarkInfo[index][0] in labelList[i][0]:\n","        similarImages.append(images[i])\n","\n","  for image in similarImages:\n","    rgbSimilarImages.append(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) # Convert the images to a RGB format so that they can be displayed properly\n","\n","  rgbSimilarImages = np.asarray(rgbSimilarImages)\n","\n","  return rgbSimilarImages"],"metadata":{"id":"YxpBsl3hssmg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test on 100 samples total, 20 of each ethnicity group\n","white_sample = predictionSample(test_white_images, 5)\n","white_predictions = model.predict(white_sample, batch_size=16)\n","white_predictions = tf.cast(white_predictions, tf.int32)\n","white_prediction_indexes = landmarkSimilarity(white_predictions, landmark_info)\n","white_similar_images = similarImages(white_prediction_indexes, cropped_image_labels, image_var, landmark_info)\n","\n","black_sample = predictionSample(test_black_images, 5)\n","black_predictions = model.predict(black_sample, batch_size=16)\n","black_predictions = tf.cast(black_predictions, tf.int32)\n","black_prediction_indexes = landmarkSimilarity(black_predictions, landmark_info)\n","black_similar_images = similarImages(black_prediction_indexes, cropped_image_labels, image_var, landmark_info)\n","\n","asian_sample = predictionSample(test_asian_images, 5)\n","asian_predictions = model.predict(asian_sample, batch_size=16)\n","asian_predictions = tf.cast(asian_predictions, tf.int32)\n","asian_prediction_indexes = landmarkSimilarity(asian_predictions, landmark_info)\n","asian_similar_images = similarImages(asian_prediction_indexes, cropped_image_labels, image_var, landmark_info)\n","\n","indian_sample = predictionSample(test_indian_images, 5)\n","indian_predictions = model.predict(indian_sample, batch_size=16)\n","indian_predictions = tf.cast(indian_predictions, tf.int32)\n","indian_prediction_indexes = landmarkSimilarity(indian_predictions, landmark_info)\n","indian_similar_images = similarImages(indian_prediction_indexes, cropped_image_labels, image_var, landmark_info)\n","\n","other_sample = predictionSample(grey_other_images, 5)\n","other_predictions = model.predict(other_sample, batch_size=16)\n","other_predictions = tf.cast(other_predictions, tf.int32)\n","other_prediction_indexes = landmarkSimilarity(other_predictions, landmark_info)\n","other_similar_images = similarImages(other_prediction_indexes, cropped_image_labels, image_var, landmark_info)"],"metadata":{"id":"sGNgOPDRIriz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for idx in range(len(white_sample)):\n","  fig, axes = plt.subplots(1,2)\n","  axes[0].imshow(white_sample[idx], cmap='gray')\n","  axes[1].imshow(white_similar_images[idx])"],"metadata":{"id":"JSWh8GISVeB2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for idx in range(len(black_sample)):\n","  fig, axes = plt.subplots(1,2)\n","  axes[0].imshow(black_sample[idx], cmap='gray')\n","  axes[1].imshow(black_similar_images[idx])"],"metadata":{"id":"NvrtUFi6B5dn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for idx in range(len(asian_sample)):\n","  fig, axes = plt.subplots(1,2)\n","  axes[0].imshow(asian_sample[idx], cmap='gray')\n","  axes[1].imshow(asian_similar_images[idx])"],"metadata":{"id":"Pr2K6D0HCCge"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for idx in range(len(indian_sample)):\n","  fig, axes = plt.subplots(1,2)\n","  axes[0].imshow(indian_sample[idx], cmap='gray')\n","  axes[1].imshow(indian_similar_images[idx])"],"metadata":{"id":"P1j24JCdCJOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for idx in range(len(other_sample)):\n","  fig, axes = plt.subplots(1,2)\n","  axes[0].imshow(other_sample[idx], cmap='gray')\n","  axes[1].imshow(other_similar_images[idx])"],"metadata":{"id":"62VZu6p8CQ1N"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","gpuClass":"premium"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"9c9e698f9ad0cf086c276d49f8b2a00e6ed7c451063c0b5e2d28206a294cd078"}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}